# -*- coding: utf-8 -*-
"""Multi-class_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ehxG8PaB44C_PDW4jQbZ_sVH96DljSAV
"""

# 1. Project Goal & Classification Type
# Main goal will be to build a model that can read the text of a support ticket (subject and/or body) and assign one or more relevant categories (labels) to it. This is crucial for automation tasks like:
# Intelligent Routing: Sending the ticket to the correct department (e.g., Billing, Technical Support, Product Feature Request).
# Prioritization: Assigning urgency (e.g., Critical, Medium, Low).
# Auto-tagging: Applying multiple descriptive tags (e.g., "Software Bug," "Login Issue," "Mobile App").
# This is a Multi-Label Classification problem because a single ticket can simultaneously belong to multiple categories (e.g., a ticket could be both "Technical Support" AND a "Billing" issue).

# import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# #1. import dataset from Kaggle
# import kagglehub

# # Download latest version
# path = kagglehub.dataset_download("tobiasbueck/multilingual-customer-support-tickets")

# print("Path to dataset files:", path)

# 1. Load the dataset
df = pd.read_csv('/content/aa_dataset-tickets-multi-lang-5-2-50-version.csv')
display(df.head())

# 2. Data Preprocessing (NLP)Text Cleaning:
# Remove special characters, URLs, HTML tags, punctuation (if necessary), and convert to lowercase.
# Tokenization: Split the text into individual words or sub-words.
# Stop Word Removal/Lemmatization: Remove common words (like 'the', 'a') and reduce words to their base form (e.g., 'running' $\rightarrow$ 'run').
# Vectorization/Embeddings: Convert the cleaned text into numerical features.
# Traditional: Use TF-IDF (Term Frequency-Inverse Document Frequency) to weigh the importance of words.

# Text Cleaning
import re
import string

def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['text'] = df['subject'].fillna('') + ' ' + df['body'].fillna('')
df['cleaned_text'] = df['text'].apply(clean_text)
display(df[['text', 'cleaned_text']].head())

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Tokenization and Stop Word Removal
stop_words = set(stopwords.words('english'))

def tokenize_and_remove_stopwords(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

df['processed_text'] = df['cleaned_text'].apply(tokenize_and_remove_stopwords)
display(df[['cleaned_text', 'processed_text']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorization using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You can adjust max_features
X = tfidf_vectorizer.fit_transform(df['processed_text']).toarray()

print("Shape of the TF-IDF matrix:", X.shape)

# Encoding the vectors
# Prepare Labels for Multi-Label Classification

# Identify the tag columns. Assuming they are named 'tag_1' to 'tag_8'
tag_columns = [col for col in df.columns if col.startswith('tag_')]

# Fill any potential NaN values in tag columns with an empty string
df[tag_columns] = df[tag_columns].fillna('')

# Get all unique tags
all_tags = sorted(list(set([tag for tags in df[tag_columns].values for tag in tags if tag != ''])))

# Create a multi-hot encoding for the tags
for tag in all_tags:
    df[tag] = df[tag_columns].apply(lambda row: int(tag in row.values), axis=1)

# Display the new columns and a sample of the data
display(df[all_tags].head())

# Get the labels matrix
y = df[all_tags].values

print("Shape of the labels matrix:", y.shape)

# 3. Split the data into train and test sets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Visualize the split
split_sizes = {'Train': X_train.shape[0], 'Test': X_test.shape[0]}
plt.figure(figsize=(6, 4))
sns.barplot(x=list(split_sizes.keys()), y=list(split_sizes.values()))
plt.title('Distribution of Data Split')
plt.ylabel('Number of Samples')
plt_ax = plt.gca()
plt_ax.bar_label(plt_ax.containers[0])
plt.show()

# 4. Train the multi-label classification model using LinearSVC classifier

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import jaccard_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Choose and train the multi-label classification model
# Using OneVsRest strategy with LinearSVC as the base classifier
model = OneVsRestClassifier(LinearSVC(random_state=42))
model.fit(X_train, y_train)

# Make predictions on the test set
y_test_pred = model.predict(X_test)

# Evaluate the model
# Jaccard Score is a common metric for multi-label classification
jaccard_accuracy_test = jaccard_score(y_test, y_test_pred, average='samples')

print(f"Jaccard Score (Sample Average) on Test Set: {jaccard_accuracy_test}")


# You can also print a classification report for more detailed metrics per label
# Note: This might be very long due to the large number of labels
# print(classification_report(y_test, y_test_pred, target_names=all_tags))

"""# Task
Perform transfer learning using a BERT model for multi-label classification on the provided dataset, evaluate its performance, and compare it with the previously trained TF-IDF model.

## Install necessary libraries

### Subtask:
Install Hugging Face Transformers and other required libraries.

**Reasoning**:
Install the necessary libraries for using Hugging Face Transformers, including transformers and torch.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers torch

"""## Load pre-trained bert model and tokenizer

### Subtask:
Choose and load a suitable pre-trained BERT model and its corresponding tokenizer.

**Reasoning**:
Import the necessary classes and load the pre-trained BERT tokenizer and model for multi-label classification.
"""

from transformers import BertTokenizer, TFBertForSequenceClassification

# Choose a pre-trained BERT model name
model_name = 'bert-base-uncased'

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name)

# Define the number of labels from the shape of y_train
# Assuming y_train is available from previous steps
num_labels = y_train.shape[1]

# Load the pre-trained BERT model for multi-label sequence classification
# Explicitly setting from_pt=False to load the TensorFlow version if available
# The default behavior should load the correct framework based on the environment,
# but explicit setting can sometimes help avoid issues.
try:
    bert_model = TFBertForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        problem_type="multi_label_classification",
        from_pt=False # Explicitly try to load the TensorFlow version
    )
except Exception as e:
    print(f"Error loading TensorFlow model directly: {e}")
    # If loading TensorFlow directly fails, try loading from PyTorch and converting
    print("Attempting to load from PyTorch and convert...")
    try:
        bert_model = TFBertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            problem_type="multi_label_classification",
            from_pt=True # Try loading from PyTorch and converting
        )
        print("Successfully loaded from PyTorch and converted.")
    except Exception as e_pt:
        print(f"Error loading from PyTorch and converting: {e_pt}")
        bert_model = None # Set to None if both attempts fail


if tokenizer and bert_model:
    print(f"Tokenizer loaded: {type(tokenizer)}")
    print(f"BERT model loaded: {type(bert_model)}")
    print(f"Number of labels set in BERT model: {bert_model.config.num_labels}")
else:
    print("Failed to load either tokenizer or model.")

"""## Prepare data for BERT

### Subtask:
Tokenize and encode the text data using the BERT tokenizer.

**Reasoning**:
Tokenize and encode the `cleaned_text` column using the loaded BERT tokenizer. This will convert the text into numerical input IDs, attention masks, and token type IDs that the BERT model expects.
"""

# Tokenize and encode the text data
# Using the 'cleaned_text' column which has already undergone basic cleaning
encoded_inputs = tokenizer(
    df['cleaned_text'].tolist(),  # Process the list of cleaned texts
    padding=True,                 # Pad sequences to the maximum length
    truncation=True,              # Truncate sequences longer than the model's max length
    max_length=128,               # Explicitly set a maximum sequence length to reduce memory usage
    return_tensors='tf'           # Return TensorFlow tensors
)

# The output 'encoded_inputs' is a dictionary-like object containing:
# 'input_ids': token IDs
# 'attention_mask': mask to avoid performing attention on padding tokens
# 'token_type_ids': (optional) for distinguishing between sequences in pair tasks

print("Keys in encoded_inputs:", encoded_inputs.keys())
print("Shape of input_ids:", encoded_inputs['input_ids'].shape)
print("Shape of attention_mask:", encoded_inputs['attention_mask'].shape)
# print("Shape of token_type_ids:", encoded_inputs['token_type_ids'].shape) # Uncomment if token_type_ids are needed

"""## Split BERT-processed data

### Subtask:
Split the encoded data and labels into training and testing sets.

**Reasoning**:
Split the `encoded_inputs` and the labels (`y`) into training and testing sets using `train_test_split`. This prepares the data for training and evaluating the BERT-based model.
"""

from sklearn.model_selection import train_test_split
import tensorflow as tf

# Convert encoded_inputs tensors to NumPy arrays for train_test_split
input_ids_np = encoded_inputs['input_ids'].numpy()
attention_mask_np = encoded_inputs['attention_mask'].numpy()
token_type_ids_np = encoded_inputs['token_type_ids'].numpy()

# Split data into training and testing sets
input_ids_train, input_ids_test, y_train_bert, y_test_bert = train_test_split(
    input_ids_np, y, test_size=0.2, random_state=42
)
attention_mask_train, attention_mask_test, _, _ = train_test_split(
    attention_mask_np, y, test_size=0.2, random_state=42
)
token_type_ids_train, token_type_ids_test, _, _ = train_test_split(
    token_type_ids_np, y, test_size=0.2, random_state=42
)


# It's often more convenient to keep the inputs as a dictionary or tuple for model training
# Convert back to TensorFlow tensors for model input
X_train_bert = {
    'input_ids': tf.constant(input_ids_train),
    'attention_mask': tf.constant(attention_mask_train),
    'token_type_ids': tf.constant(token_type_ids_train)
}
X_test_bert = {
    'input_ids': tf.constant(input_ids_test),
    'attention_mask': tf.constant(attention_mask_test),
    'token_type_ids': tf.constant(token_type_ids_test)
}


print("Shape of input_ids_train:", X_train_bert['input_ids'].shape)
print("Shape of attention_mask_train:", X_train_bert['attention_mask'].shape)
print("Shape of token_type_ids_train:", X_train_bert['token_type_ids'].shape)
print("Shape of y_train_bert:", y_train_bert.shape)

print("\nShape of input_ids_test:", X_test_bert['input_ids'].shape)
print("Shape of attention_mask_test:", X_test_bert['attention_mask'].shape)
print("Shape of token_type_ids_test:", X_test_bert['token_type_ids'].shape)
print("Shape of y_test_bert:", y_test_bert.shape)

"""## Build and train the transfer learning model

### Subtask:
Construct a multi-label classification model by adding a classification layer on top of the pre-trained BERT model and train it on the prepared data.

**Reasoning**:
Define a TensorFlow model using the loaded `bert_model`, add a dense layer with a sigmoid activation for multi-label classification, compile the model with an appropriate loss function and optimizer, and then train the model using the prepared training data.
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Define the BERT model with a classification layer
# Use the pre-trained bert_model (TFBertForSequenceClassification) which already has a classification head
# We can directly use this model for fine-tuning
model = bert_model

# Compile the model
# Using BinaryCrossentropy with from_logits=True because the output layer likely doesn't have an activation yet,
# or if it does (like sigmoid), using from_logits=True is numerically more stable.
# Given problem_type="multi_label_classification" was set during loading,
# the model's output layer is expected to be suitable for this, likely without an activation initially.
model.compile(
    optimizer='adam', # Pass optimizer as a string
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[tf.metrics.BinaryAccuracy()]
)

# Train the model
# Using X_train_bert (dictionary of tensors) and y_train_bert (numpy array)
# Epochs and batch size can be adjusted
history = model.fit(
    X_train_bert,
    y_train_bert,
    epochs=2,  # You can adjust the number of epochs
    batch_size=16, # Reduced batch size to mitigate OOM error
    validation_data=(X_test_bert, y_test_bert) # Use test set for validation during training
)

"""## Evaluate the BERT model

### Subtask:
Evaluate the performance of the transfer learning model on the test set.

**Reasoning**:
Evaluate the trained BERT model on the test set (`X_test_bert`, `y_test_bert`) using appropriate multi-label classification metrics, such as the Jaccard score and potentially a classification report.
"""

from sklearn.metrics import jaccard_score, classification_report
import numpy as np

# Make predictions on the test set
# The output of the BERT model is logits, so we need to apply a sigmoid activation
test_predictions = model.predict(X_test_bert)
test_predictions_sigmoid = tf.sigmoid(test_predictions.logits).numpy()

# Apply a threshold to convert probabilities to binary predictions (0 or 1)
# You can adjust the threshold based on your needs
threshold = 0.5
y_test_pred_bert = (test_predictions_sigmoid > threshold).astype(int)

# Evaluate the model
# Jaccard Score (Sample Average)
jaccard_accuracy_bert = jaccard_score(y_test_bert, y_test_pred_bert, average='samples')

print(f"BERT Model Jaccard Score (Sample Average) on Test Set: {jaccard_accuracy_bert}")

# You can also print a classification report
# Note: This can be very long due to the number of labels
# print(classification_report(y_test_bert, y_test_pred_bert, target_names=all_tags))

"""## Compare Model Predictions

### Subtask:
Compare the predictions of the BERT model and the TF-IDF model on the test set.

**Reasoning**:
Compare the prediction matrices from the TF-IDF model (`y_test_pred`) and the BERT model (`y_test_pred_bert`) to the true labels (`y_test_bert`). Calculate and display metrics that show the extent of agreement and disagreement between the models' predictions.
"""

import numpy as np

# Assuming y_test_pred (from TF-IDF model) and y_test_pred_bert (from BERT model) are available
# Also assuming y_test_bert (true labels for the test set) is available

# Ensure the true labels are the same for both comparisons (they should be if split with same random state)
# assert np.array_equal(y_test, y_test_bert) # uncomment to check if using y_test from TF-IDF split

# Calculate the number of samples where both models predicted the same labels
agreement_count = np.sum(np.all(y_test_pred == y_test_pred_bert, axis=1))
total_samples = y_test_bert.shape[0]

print(f"Number of samples where both models made the exact same multi-label predictions: {agreement_count} out of {total_samples}")
print(f"Percentage of samples with exact agreement: {agreement_count / total_samples * 100:.2f}%")

# You can also look at disagreements for a few examples
disagreement_indices = np.where(~np.all(y_test_pred == y_test_pred_bert, axis=1))[0]

print(f"\nShowing predictions for a few samples where the models disagreed:")
for i in disagreement_indices[:5]: # Display first 5 disagreements
    print(f"\nSample Index: {i}")
    print(f"True Labels: {np.where(y_test_bert[i] == 1)[0]}")
    print(f"TF-IDF Model Predictions: {np.where(y_test_pred[i] == 1)[0]}")
    print(f"BERT Model Predictions: {np.where(y_test_pred_bert[i] == 1)[0]}")

    # To see the actual tags, you would need the all_tags list
    # print(f"True Tags: {[all_tags[j] for j in np.where(y_test_bert[i] == 1)[0]]}")
    # print(f"TF-IDF Predicted Tags: {[all_tags[j] for j in np.where(y_test_pred[i] == 1)[0]]}")
    # print(f"BERT Predicted Tags: {[all_tags[j] for j in np.where(y_test_pred_bert[i] == 1)[0]]}")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming jaccard_accuracy_test (TF-IDF) and jaccard_accuracy_bert (BERT) are available

model_names = ['TF-IDF Model', 'BERT Model']
jaccard_scores = [jaccard_accuracy_test, jaccard_accuracy_bert]

plt.figure(figsize=(8, 5))
sns.barplot(x=model_names, y=jaccard_scores)
plt.title('Comparison of Model Performance (Jaccard Score)')
plt.ylabel('Jaccard Score (Sample Average)')
plt.ylim(0, 1) # Jaccard score is between 0 and 1

# Add the score values on top of the bars
plt_ax = plt.gca()
plt_ax.bar_label(plt_ax.containers[0], fmt=':.4f')

plt.show()

"""## Summary: Key Differences Between TF-IDF and BERT Models

Based on the process and results in this notebook, here's a summary of the key differences between the TF-IDF and BERT models for multi-label text classification:

1.  **Text Representation:**
    *   **TF-IDF:** A statistical method creating sparse, high-dimensional vectors based on word frequencies, treating words independently.
    *   **BERT:** A pre-trained transformer model learning dense, contextual embeddings that capture semantic and syntactic relationships, leveraging transfer learning.

2.  **Model Complexity and Training:**
    *   **TF-IDF + LinearSVC:** Simpler and faster, requires less computational resources.
    *   **BERT (Transfer Learning):** More complex, requires significant computational resources (GPU) and longer training times for fine-tuning.

3.  **Handling of Language:**
    *   **TF-IDF:** Language-dependent, typically requires language-specific preprocessing.
    *   **BERT:** Can handle multiple languages and capture cross-lingual relationships (especially multilingual BERT variants), suitable for multilingual datasets.

4.  **Performance (based on Jaccard Score in this notebook):**
    *   TF-IDF Model Jaccard Score on Test Set: {{jaccard_accuracy_test:.4f}}
    *   BERT Model Jaccard Score on Test Set: {{jaccard_accuracy_bert:.4f}}

In conclusion, while TF-IDF is a solid baseline, the BERT model, with its ability to understand context and leverage pre-trained knowledge, often achieves better performance on complex text tasks, albeit at a higher computational cost.
"""